<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>NLPrompt</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <span style="background: -webkit-linear-gradient(left, rgb(170, 102, 153), rgb(102, 136, 187)); -webkit-background-clip: text; color: transparent;">NLPrompt</span>
              :
              <span style="background : -webkit-linear-gradient(left, rgb(170, 102, 153), rgb(102, 136, 187)); -webkit-background-clip: text; color: transparent;">N</span>oise-<span style="background: -webkit-linear-gradient(left, rgb(170, 102, 153), rgb(102, 136, 187)); -webkit-background-clip: text; color: transparent;">L</span>abel
              <span style="background: -webkit-linear-gradient(left, rgb(170, 102, 153), rgb(102, 136, 187)); -webkit-background-clip: text; color: transparent;">Prompt</span> Learning<br> for Vision-Language Models
            </h1>
            <h2 class="subtitle is-3 has-text-centered"
              style="color: #cc3434; font-weight: bold; margin-top: 0.5em;">
              &#127942; CVPR 2025 Highlight
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://panbikang.github.io/home/">Bikang Pan</a><sup>1,†</sup>,
              </span>
              <span class="author-block">
                <a>Qun Li</a><sup>1,†</sup>,
              </span>
              <span class="author-block">
                <a>Xiaoying Tang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a>Wei Huang</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a>Zhen Fang</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a>Feng Liu</a><sup>5</sup>,
              </span>
              <span class="author-block">
                <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Jingyi Yu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://shiye21.github.io/">Ye Shi</a><sup>1,*</sup>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>ShanghaiTech University, Shanghai, China</span><br>
              <span class="author-block"><sup>2</sup>The Chinese University of Hong Kong, Shenzhen, China</span><br>
              <span class="author-block"><sup>3</sup>RIKEN Center for Advanced Intelligence Project, Japan</span><br>
              <span class="author-block"><sup>4</sup>University of Technology Sydney, Australia</span>&nbsp;&nbsp;
              <span class="author-block"><sup>5</sup>University of Melbourne, Australia</span><br>
              <!-- <span class="author-block" style="color: #cc3434; font-weight: bold; font-size: 1.5em;">&#127942; CVPR 2025 Highlight</span><br> -->
              <span class="author-block" style="font-size: 0.8em;"><sup>†</sup>Indicates Equal Contribution</span>&nbsp;&nbsp;
              <span class="author-block" style="font-size: 0.8em;"><sup>*</sup>Indicates Corresponding Author</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models_CVPR_2025_paper.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.01256" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/qunovo/NLPrompt" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <!-- <h2 class="title is-3">A Simple and Effective Framework for Learning with Noisy Labels in VLMs</h2> -->
         <p>
              <img src="static\images\5.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
              We addressed the critical challenge of noisy labels in prompt learning for vision-language foundation models by introducing PromptMAE and PromptOT.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text features in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representations and precise alignment capabilities of vision-language models for robust prompt learning. We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Youtube video -->
<section class="hero is-small is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Video (coming soon)</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3 has-text-centered">Motivation </h2>
          <p>
            In the realm of noisy label learning, mean absolute error (MAE) has been identified as a robust loss function within the traditional training paradigm. However, MAE often suffers from slow convergence and poor performance during training, making it seldom employed as a classification loss in noise-label learning. <br>Nevertheless, our investigation reveals an interesting phenomenon: employing MAE loss in Prompt learning (PromptMAE) notably enhances robustness while maintaining high accuracy compared to traditional cross-entropy loss. As shown in Figure, MAE exhibits strong accuracy and fast convergence even in the presence of substantial noise. 
          </p>
          <p>
            <img src="static\images\6.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
          </p>
          <p>
            To elucidate PromptMAE's robustness, we leverage feature learning theory to demonstrate that it can suppress the influence of noisy samples, thereby enhancing the robustness of prompt learning in vision–language models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3"> Comparison of Exisiting 3D Affordance Dateset with Ours. </h2>
         <p>
              <img src="static\images\fig3.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
              #Point Cloud and #Instruction-Point Cloud Pairs denote the number of point clouds and instruction-point cloud pairs, respectively. X  indicates that the dataset does not possess this attribute.         </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3"> SeqAfford Method </h2>
         <p>
              <img src="static\images\fig4.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
              Given the point clouds of the target objects and a piece of complex human instruction, SeqAfford first reasons from this instruction and decomposes it into several hidden [SEG] tokens extracted from the last-layer embeddings, each representing an intermediate affordance segmentation result. Then, for each [SEG], the point features extracted by the 3D vision encoder dynamically interact with the [SEG] token before being sent to the decoder for mask generation. The interaction is achieved through multi-granular language-point integration, synergizing both reasoning and affordance segmentation. We use LoRA for efficient fine-tuning.      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3"> Multi-Granular Language-Point Integration Module. </h2>
         <p>
              <img src="static\images\fig5.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
              We propose an interaction module between [SEG] tokens from LLM and point features from the 3D vision encoder, to synergize both reasoning and segmentation in a cohesive framework. This module consists of the multi-granular feature propagation process, and the point-language integration stage.    </div>
  </div>
</section> -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3 has-text-centered">Experiments Results </h2>
          <h3 class="subtitle is-5 " style="margin-top: 0.5em;">
            On the synthetic noisy datasets
          </h3>
          <p style="text-align: center;">
            <img src="static\images\1.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
          </p>
          <p>
            For the synthetic noisy dataset, the accuracy of the image classification task under different noise levels is shown in the table below, demonstrating the effectiveness and superiority of NLPrompt in addressing the noisy label problem in prompt learning.          
          </p>
          <h3 class="subtitle is-5 " style="margin-top: 0.5em;">
            On the real-world noisy dataset
          </h3>
          <p style="text-align: center;">
            <img src="static\images\2.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 50%; height: auto;" />
          </p>
          <p>
            The results on the real-world noisy dataset Food101N are shown in the table below, where NLPrompt outperforms all the baseline methods.          
          </p>
          <h3 class="subtitle is-5 " style="margin-top: 0.5em;">
            Generalization of NLPrompt
          </h3>
          <p style="text-align: center;">
            <img src="static\images\3.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 60%; height: auto;" />
          </p>
          <p>
            Our method NLPrompt is effective not only for CoOp but also for other prompt learning methods, such as VPT, MaPLe, and PromptSRC, which are subsequent methods of CoOp. NLPrompt significantly improves the robustness of various prompt learning methods in the face of noisy label problems, which verifies the strong generalization ability of NLPrompt.          
          </p>
          <h3 class="subtitle is-5 " style="margin-top: 0.5em;">
            Ablation Studies
          </h3>
          <p>To evaluate the effectiveness of each component of our method, we conduct ablation studies on the Flowers102 dataset. </p>
          <p style="text-align: center;">
            <img src="static\images\4.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 60%; height: auto;" />
          </p>
          <p>
            The experimental design is as follows:  <br>
            (a) Use CE loss for all data; <br>
            (b) Use MAE loss for all data; <br>
            (c) Use random initialization prototype instead of CLIP text feature as initialization; <br>
            (d) Use CE loss for clean data only after removing noisy data; <br>
            (e) Use MAE loss for noisy data only after removing clean data.  <br>

            The average results show that (b) outperforms (a), validating the effectiveness of our PromptMAE. 
            Moreover, the average results show that (d) outperforms (a), and (e) outperforms (b), further validating the effectiveness of PromptOT in the data purification process. Additionally, the comparison between (c) and NLPrompt highlights the importance of text feature initialization in our method. <br>
            Among all methods, our NLPrompt achieves the best performance, with significant improvements over other baselines, further validating the effectiveness of each component.           
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div> -->
<!-- </section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@inproceedings{pan2025nlprompt,
    title={NLPrompt: Noise-Label Prompt Learning for Vision-Language Models},
    author={Pan, Bikang and Li, Qun and Tang, Xiaoying and Huang, Wei and Fang, Zhen and Liu, Feng and Wang, Jingya and Yu, Jingyi and Shi, Ye},
    booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
    pages={19963--19973},
    year={2025}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p style="font-size:0.8rem;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
